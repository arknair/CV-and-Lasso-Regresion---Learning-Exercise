---
title: "CV and Lasso Regression - Learning Exercise"
author: "Arjun Remeshkumar Nair"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Loading required packages
library(skimr)
library(dplyr)
library(caret)
library(glmnet)

#Setting working directory for data files
knitr::opts_knit$set(root.dir = '')
```

```{r Question 1}
### Question 1
#Reading the dataset
heart <- read.csv("/heart.csv")

#Observing the basic structure and summary statistics of the dataset
str(heart)
summary(heart)
skim(heart)

sprintf("From the above summary statistics, we observe that the columns family_record and past_record are completely NA. The column wrist.dim has about 238 missing values of a total of 240 values. A few other columns have less than 5 missing values at most, which is insignificant compared to the total of 240 rows that we have. These 3 columns can be removed from further analysis.")

#Removing family_record, past_record and wrist.dim columns
heart <- heart[,-c(7,11,15)]

#Replacing missing values with mean of the column
heart$height[is.na(heart$height)] <- mean(heart$height, na.rm=TRUE)
heart$fat_free_wt[is.na(heart$fat_free_wt)] <- mean(heart$fat_free_wt, na.rm=TRUE)
heart$chest_dim[is.na(heart$chest_dim)] <- mean(heart$chest_dim, na.rm=TRUE)
heart$hip_dim[is.na(heart$hip_dim)] <- mean(heart$hip_dim, na.rm=TRUE)
heart$thigh_dim[is.na(heart$thigh_dim)] <- mean(heart$thigh_dim, na.rm=TRUE)
heart$biceps_dim[is.na(heart$biceps_dim)] <- mean(heart$biceps_dim, na.rm=TRUE)

sprintf("While choosing a sample subset from this data, a few things to be considered are:")
sprintf("1. Sample should be representative of the overall data")
sprintf("2. The split for Training/Test should be considered ( usually 80/20)")
sprintf("3. Avoid missing values when possible")
sprintf("4. There should be a balance between the treated and untreated observations")

#Setting the seed 
set.seed(2356)

#Creating a data partition
partition <- createDataPartition(y = heart$heart_attack, p = 0.8, list = FALSE)

#Defining the training and test sets
trainset <- heart[partition, ]
testset <- heart[-partition, ]

#Running a full linear regression model with heart_attack as the response variable
linearmodel <- glm(heart_attack ~ .,data = trainset)

#Exploring the model statistics
summary(linearmodel)

sprintf("At a 0.05 level of significance, we observe 10 variables that are signficant and can be included in the model.")
sprintf("heart_attack = 0.032*past_pain + 0.058*weight - 0.151*height - 0.06*fat_free_wt + 0.129*neck_dim + 0.115*chest_dim + 0.11*abdom_dim + 0.0776*thigh_dim - 0.175*knee_dim + 0.106*ankle_dim ")

#Testing the full model on the test dataset
testdata_prediction <- predict(linearmodel, testset)

#Calculating the OOS R-squared value
data.frame(R2 = R2(testdata_prediction, testset$heart_attack), RMSE = RMSE(testdata_prediction, testset$heart_attack), MAE = MAE(testdata_prediction, testset$heart_attack))

sprintf("We observe that the R-squared value for the predictions of the test data is 0.914")
```

```{r Question 2}
### Question 2
sprintf("Cross-validation is a technique for evaluating the performance of a model by using train and test datasets. The common methods of cross-validations are Validation set approach(Train vs Test split in the data), Leave one out cross validation, k-fold cross validation and repeated k-fold cross validation.")

sprintf("A common method is using k-fold cross validation, where the model is trained using k-1 subsets and then tested on the remaining subset. The process is repeated k times, with different subsets being used for testing each time. The average of the various iterations is considered as the model performance for out-of-sample data. " )

sprintf("A common problem associated with cross-validation is that, if the samples chosen for train and test are not equally representative, the model might be overfitted and this could lead to an overestimation of the model performance. This method of cross validation as well as the repeated k-fold method could be expensive in terms of resources needed, mainly for comple models or large datasets. There could be issues in cases where the data is spatial or temporal in nature.  ")
```

```{r Question 3}
### Question 3

#Defining Training Control 
train_control <- trainControl(method = "cv", number = 8)

#Creating a cross-validated model on the training data created in Q1
crossvalidation_model <- train(heart_attack ~ ., data = trainset, method = "glm", trControl = train_control)

#Observing the model statistics
summary(crossvalidation_model)
print(crossvalidation_model)

#Using the cross-validation_model to predict on the test_data
predict_cross_model <- predict(crossvalidation_model, testset)

#Calculating the OOS Null Deviance
null_dev <- sum((testset$heart_attack - mean(testset$heart_attack)) ^2)

#Calculating the OOS Residual Deviance
resid_dev <- sum((testset$heart_attack - predict_cross_model) ^2)

#Calculating the OOS R-squared
cross_validation_rsq <- 1 - (resid_dev/null_dev)

sprintf("We observe that the final R-squared value created from the cross-validation model is 0.913.")

sprintf("The Rsquared value from Q1 is 0.914 and the cross-validated model is 0.913. Judging by the Rsquared values, the initial model appears to be a better choice. The higher R-squared value can be explained by the multi-fold cross validation technique used on the training data.")
```

```{r Question 4}
### Question 4

sprintf("Lasso stands for Least Absolute Shrinkage and Selection Operator. Lasso Regression is different in the fact that the regression equation will include a penalty term that is applied to the coefficients, by shrinking some of the coefficients, to prevent overfitting. This penalty term is called the regularization term.")

sprintf("The algorithm works by minimizing the sum of squared residuals, while following the rule of keeping the absolute value of coefficients less than the regularization value. This usually forces the values of some coefficients to drop to near 0, essentially eliminating the feature from the model.")

sprintf("The pros are :")
sprintf("1. It helps to prevent overfitting by automatically doing feature selection.")
sprintf("2. It helps reduce model complexity by removing certain features that it does not seem neccessary.")
sprintf("3. It can work for continous data, binomial data and when there is high correlation between independent variables.")

sprintf("The cons are :")
sprintf("1. Since the coefficients have been shrunk, the model coefficients do not showcase the exact relationship between the features and the outcome.")
sprintf("2. It produces unstable estimates, meaning that different features could get dropped if the data is bootstrapped.")
sprintf("3. In highly correlated data, the created model could be unstable.")
sprintf("4. It cannot handle categorical variables.")
sprintf("5. It cannot be used if the primary objective is prediction.")
```

```{r Question 5}
### Question 5

#Part 1
#Defining the response variable
y <- trainset$heart_attack

#Defining a matrix for the predictor variables
x <- data.matrix(trainset[,c('past_pain','density','age','weight','height','fat_free_wt','neck_dim','chest_dim','abdom_dim','hip_dim','thigh_dim','knee_dim','ankle_dim', 'biceps_dim','forearm_dim','wrist_dim.1')])

#Running a lasso regression model which performs cross-validation
lassomodel <- cv.glmnet(x, y, alpha = 1)

#Plotting MSE by lambda value
plot(lassomodel)

#Observing the lambda values from the cross-validated model
lambda_min <- lassomodel$lambda.min
lambda_1se <- lassomodel$lambda.1se

#Fitting the regression models using both the lambda values obtained above
lasso_min_model <- glmnet(x, y, alpha = 1, lambda = lambda_min)
print(lasso_min_model)
lasso_1se_model <- glmnet(x, y, alpha = 1, lambda = lambda_1se)
print(lasso_1se_model)

testset_x <- testset[,-17]

#Using the lambda_1se_model to predict on the test_data
predict_lasso_1se_model <- predict(lasso_1se_model, s = lambda_1se, newx = as.matrix(testset_x))

#Calculating the OOS Null Deviance
null_dev <- sum((testset$heart_attack - mean(testset$heart_attack)) ^2)

#Calculating the OOS Residual Deviance
resid_dev <- sum((testset$heart_attack - predict_lasso_1se_model) ^2)

#Calculating the OOS R-squared
lasso_1se_rsq <- 1 - (resid_dev/null_dev)

sprintf("The model using lambda_min will give us the minimum cross-validated error and the model using lambda_1se will give the smallest error within 1 Standard Error of the minimum.")

sprintf("I would choose the model using lambda_1se since it is less likely overfit the data.")

#Part 2
#Comparing the R-squared values for Q11, Q3 and Q5
sprintf("The R-squared value for Q1 = 0.914")
sprintf("The R-squared value for Q3 = 0.913")
sprintf("The R-squared value for Q5 = 0.815")

sprintf("The best R-squared value is observed for the initial model. The R-squared values for the k-fold cross-validated model and linear regression model are similar.")

```

```{r Question 6}
### Question 6

sprintf("AIC (Akaike Information Criterion) is model selection criteria used to compare the quality of models. It provides a score after consiering the complexity of the model and the goodness of fit. The model with the minimum AIC value is preferred. AIC indicates the quality of a model as compared to other models, but not the absolute quality of a model. It is recommended to validate the quality of the model after computing AIC.")
sprintf("AIC is usually calculated as  AIC = 2*k - 2*ln(L), where L is the likelihood of the data given the model and k is the # of parameters.")

sprintf("AICc (Corrected AIC) is a modified version of the AIC and it is used when the sample size of the data is small compared to the # of parameters. We can observe from the equation that as n -> infinity, AICc will converge to AIC.")
sprintf(" It is calculated as AICc = AIC + (2*k*(k+1))/(n-k-1), where n is the sample size, k is the # of parameters.")

```